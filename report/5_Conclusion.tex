\section{Future Work \& Conclusion}

	\subsection{Possible Improvements}
	
	\subsubsection{Wasserstein GAN}
	One architectural concept that has turned out to be able to introduce sensible improvements is called Wasserstein GAN (WGAN)\cite{arjovsky2017wasserstein}. To measure the distance between the real distribution and the model distribution, the 1-Wasserstein distance is used instead of the Jensen-Shannon divergence. Our loss function must therefore comply with the Lipschütz constraint. This is achieved in the original paper by weight clipping, the process of keeping the value within a certain range. \\
	This gives two major advantages, one of which is that modal collapse is much less common. Also, it is generally easier to train the discriminator than the generator. In the worst case, the generator no longer learns because of the lack of balance between the two networks. WGANs help to decouple them to a certain extent, therefore training becomes more stable. \\
	Another enhancement is the WGAN with a gradient penalty. It uses gradient penalties instead of weight clipping to satisfy the Lipschütz constraint. This is a good idea since weight clipping was even described by the inventors of the WGANs as a terrible idea\cite{arjovsky2017wasserstein}, because it heavily confines the value of the weights. WGAN with gradient penalty tends to enhance training stability ever further. \\
	\subsubsection{Randomized Leaky ReLU}
	Although some problems have been solved with the introduction of Leaky ReLU, there is still room for improvement. Bing Xu et al. \cite{xu2015empirical} published a paper in 2015, which explains their evaluations of the different types of rectified linear unit. They used ReLU as the baseline, with the variations Leaky ReLU, Parametric ReLU and Randomized Leaky ReLU. The normal version of Leaky ReLU and its advantages were already covered in Chapter 3. Parametric ReLU corresponds to Leaky ReLU in a modified form, because the parameter alpha is not given, but is learned during training by back propagation. Randomized Leaky ReLU samples alpha from a uniformed distribution which is determined by a given lower and upper bound. While Parametric ReLU according to the findings of Bing Xu et al. suffers from overfitting, Randomized Leaky Relu reduces this by introducing coincidences. As far as we know, there is no Randomized Leaky ReLU activation function available in Keras. Therefore, by implementing and using Randomized Leaky ReLU as a custom activation function in our project we can try to validate their results. We have to say, however, that we do not know of any other research related to the comparison of the ReLU that considers Randomized Leaky ReLU as a viable option.
    \\
    \subsubsection{Dataset}
    As already mentioned, the dataset is not particularly large, but has fire as its dominant feature. To get better results, we
    suggest three starting points. The most obvious way is to collect more pictures which can be approached via a web search. Due to tragic disasters like the bushfires in Australia, more pictures will, unfortunately, surface for sure. Since there is no public database for those kind of pictures, one would have to resort to private databases for ordered data. Insurance companies may have saved such images, but it is very unlikely that they will be made accessible for data protection reasons. \\
    Searching for images on online platforms that collect satellite images like ESA online\footnote{https://sentinel.esa.int/web/sentinel/home} also turn out to be difficult. Since fires would have to be matched on location and time in order to succeed in finding usable data. In addition, it is unlikely to discover images in an appropriate format. GANs are bad at dealing with pictures from different angles and are not able to comprehend 3D representations yet. \\
    The second change is no less intricate, but can significantly improve the quality of the results. As mentioned, the average size is 828X1244. That may be reduced by cutting off unnecessary peripheral features such as firefighters, spectators or cars if not done already. In extreme cases, oversized images can be removed from the data set. \\
    The third point is probably the easiest one: letting the model run on a powerful machine. When training on our computer, after taking the load into account, we set the upper limit of the images to a size of 200X200 pixels. We will take this approach to heart during further training and consider falling back on cloud providers

	\subsection{Learning Outcome}
	Overall, carrying out the project was absolutely an enrichment for us. On one hand, we were able to dive deeper into the Keras API. Additionally, we have gained a lot of new insights into image processing and presentation. We have not had a project of this size that we were responsibly for on our own, so we could grasp on how to handle the structuring. We also learned how to create a minimum viable product for the progress we made so far.
	
	\subsection{Final Thoughts}
	First of all, we are grateful to be able to live in a time of continuous technological progress and to be part of it. The fact that we can convert the accumulated theory into problem solving-oriented technologies makes us all the happier. This paper is not only intended to provide information about a state of the art technology, but also to inspire people to try to tackle one of the many problems we face and thus to make a contribution back to society. 
	
	\appendix
    The code for the project can be found here:\\
    \url{https://github.com/andreasbinder/AppliedML\_using\_GAN} 